{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18791f0c",
   "metadata": {},
   "source": [
    "The news story in 2021 that captured the complete attention of the financial press was the [Gamestop / WallStreetBets / RoaringKitty episode](https://www.cbsnews.com/news/gamestop-reddit-wallstreetbets-short-squeeze-2021-01-28/) of late January. A group of presumably small, retail traders banded together on Reddit's [r/wallstreetbets](https://www.reddit.com/r/wallstreetbets/) forum to drive the price of `$GME`, `$AMC` and other \"meme stocks\" to unimaginable heights, wreaking havoc with the crowd of hedge funds who had shorted the stocks. \n",
    "\n",
    "In the wake of that headline-grabbing incident, many a hedge fund has begun to consider social media buzz - especially on \"meme stocks\" as a risk factor to consider when taking large positions, especially short ones. The smartest funds are going beyond simply hand-wringing and are starting to monitor social media forums like [r/wallstreetbets](https://www.reddit.com/r/wallstreetbets/) to identify potential risks in their portfolios.\n",
    "\n",
    "Below, I'm going to walk through an example of collecting `r/wallstreetbets` activity on a handful of example stocks using Reddit's semi-unofficial [PushShift API](https://github.com/pushshift/api) and related packages. In a following post, I'll walk through a simple example of sentiment analysis using [VADER](https://github.com/cjhutto/vaderSentiment), and other assorted python packages. \n",
    "\n",
    "If you'd like to experiment with the below code without tedious copy-pasting, I've made it available on the below Google Colab link. \n",
    "\n",
    "<a style=\"text-align: center;\" href=\"https://colab.research.google.com/drive/1yf4fje7IFhI0skAXddTUx0nsPbCgJgTc?usp=sharing\"><img src=\"images/colab.png\" title=\"ipynb on Colab\" /></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6193b",
   "metadata": {},
   "source": [
    "## Setup and Download\n",
    "We will access the PushShift API through a python package named [psaw](https://github.com/dmarx/psaw) (acronym for \"PushShift API Wrapper\") so first we'll need to pip install that. If you don't already have the fantastically useful `jsonlines` package installed, it'd be a good idea to install that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc76bfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psaw in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.1.0)\n",
      "Requirement already satisfied: Click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from psaw) (8.0.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from psaw) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->psaw) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->psaw) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->psaw) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->psaw) (2.10)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: jsonlines in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psaw\n",
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580ebce",
   "metadata": {},
   "source": [
    "The imports are dead-simple. I'll import pandas as well since that's my swiss army knife of choice. I'm also going to define a data root path to where on my system I want to store the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef4feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_ROOT = '../data/'\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07f023",
   "metadata": {},
   "source": [
    "## PushShift and `psaw` Overview\n",
    "\n",
    "I'll start with a quick example of how to use the psaw wrapper. You'll want to refer to the [psaw](https://github.com/dmarx/psaw) and [PushShift](https://github.com/pushshift/api) GitHub pages for more complete documentation. \n",
    "\n",
    "First, we will use the `search_submissions` API method, which searches submissions (the initial post in a new thread) for the given ticker. We need to pass in unix-type integer timestamps rather than human-readable ones, so here we're using pandas to do that. \n",
    "\n",
    "You'll also notice the `filter` parameter, which allows you to return only a subset of the many fields available. If you want to see the full list of available fields, read the docs or run the below code snippet. \n",
    "\n",
    "`\n",
    "gen = api.search_submissions(q='GME',limit=1)\n",
    "list(gen)[0].d_.keys()\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ff74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = int(pd.to_datetime('2021-01-01').timestamp())\n",
    "end_epoch = int(pd.to_datetime('2021-01-02').timestamp())\n",
    "\n",
    "gen = api.search_submissions(q='GME', # this is the keyword (ticker symbol) for which we're searching\n",
    "                               after=start_epoch, before=end_epoch, # these are the unix-based timestamps to search between\n",
    "                               subreddit=['wallstreetbets','stocks'], # one or more subreddits to include in the search\n",
    "                               filter=['id','url','author', 'title', 'score',\n",
    "                                       'subreddit','selftext','num_comments'], # list of fields to return\n",
    "                               limit = 2 # limit on the number of records returned\n",
    "                              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313daaa6",
   "metadata": {},
   "source": [
    "You'll notice that this ran awfully quickly. In part, that's due to the fact that it has returned a lazy _generator_ object which doesn't (yet) contain the data we want. One simple way to make the generator object actually pull the data is to wrap it in a `list()` call. Below is an example of what that returns. \n",
    "\n",
    "_Side note: if you don't catch the resulting list in the first time you run this, you'll notice that it won't work a second time. The generator has been \"consumed\" and emptied of objects. So we will catch the returned value in a variable called `lst` and view that..._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d0842d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[submission(author='Alexbuildit', created_utc=1609541557, id='kol20h', num_comments=2, score=1, selftext=\"Brand new investor here. Saw all the hype surrounding GME, and bought in with this months paycheck. Had a good laugh when I saw the reddit award &lt; GME post. Already down over a hundred dollars on GME, but not gonna sell! Let's send GME to the moon! I'll keep picking up GME whenever I can. ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€\\n\\n[Yep. Checks out.](https://preview.redd.it/i6wj8ynnvs861.png?width=804&amp;format=png&amp;auto=webp&amp;s=69a0688bddce3cf409caea5adde58421e60296fc)\", subreddit='wallstreetbets', title='WSB In A Nutshell. Send GME To The Moon! ðŸš€', url='https://www.reddit.com/r/wallstreetbets/comments/kol20h/wsb_in_a_nutshell_send_gme_to_the_moon/', created=1609566757.0, d_={'author': 'Alexbuildit', 'created_utc': 1609541557, 'id': 'kol20h', 'num_comments': 2, 'score': 1, 'selftext': \"Brand new investor here. Saw all the hype surrounding GME, and bought in with this months paycheck. Had a good laugh when I saw the reddit award &lt; GME post. Already down over a hundred dollars on GME, but not gonna sell! Let's send GME to the moon! I'll keep picking up GME whenever I can. ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€\\n\\n[Yep. Checks out.](https://preview.redd.it/i6wj8ynnvs861.png?width=804&amp;format=png&amp;auto=webp&amp;s=69a0688bddce3cf409caea5adde58421e60296fc)\", 'subreddit': 'wallstreetbets', 'title': 'WSB In A Nutshell. Send GME To The Moon! ðŸš€', 'url': 'https://www.reddit.com/r/wallstreetbets/comments/kol20h/wsb_in_a_nutshell_send_gme_to_the_moon/', 'created': 1609566757.0}),\n",
       " submission(author='luncheonmeat79', created_utc=1609538748, id='kok7my', num_comments=57, score=1, selftext='', subreddit='wallstreetbets', title='Ryan Cohen-GME confirmation bias', url='https://i.redd.it/xv2ie5t8ns861.jpg', created=1609563948.0, d_={'author': 'luncheonmeat79', 'created_utc': 1609538748, 'id': 'kok7my', 'num_comments': 57, 'score': 1, 'selftext': '', 'subreddit': 'wallstreetbets', 'title': 'Ryan Cohen-GME confirmation bias', 'url': 'https://i.redd.it/xv2ie5t8ns861.jpg', 'created': 1609563948.0})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = list(gen)\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5cfff",
   "metadata": {},
   "source": [
    "Each element of the returned list is a `submission` object which, as far as I can tell, simply provides easier access to the fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29914fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: kol20h\n",
      "url: https://www.reddit.com/r/wallstreetbets/comments/kol20h/wsb_in_a_nutshell_send_gme_to_the_moon/\n",
      "author: Alexbuildit\n",
      "title: WSB In A Nutshell. Send GME To The Moon! ðŸš€\n",
      "score: 1\n",
      "subreddit: wallstreetbets\n",
      "num_comments: 2\n",
      "selftext: Brand new investor here. Saw all the hype surrounding GME, and bought in with this months paycheck. Had a good laugh when I saw the reddit award &lt; GME post. Already down over a hundred dollars on GME, but not gonna sell! Let's send GME to the moon! I'll keep picking up GME whenever I can. ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€\n",
      "\n",
      "[Yep. Checks out.](https://preview.redd.it/i6wj8ynnvs861.png?width=804&amp;format=png&amp;auto=webp&amp;s=69a0688bddce3cf409caea5adde58421e60296fc)\n"
     ]
    }
   ],
   "source": [
    "print(\"id:\",lst[0].id) # this is Reddit's unique ID for this post\n",
    "print(\"url:\",lst[0].url) \n",
    "print(\"author:\",lst[0].author) \n",
    "print(\"title:\",lst[0].title)\n",
    "print(\"score:\",lst[0].score) # upvote/downvote-based score, doesn't seem 100% reliable\n",
    "print(\"subreddit:\",lst[0].subreddit)\n",
    "print(\"num_comments:\",lst[0].num_comments) # number of comments in the thread (which we can get later if we choose)\n",
    "print(\"selftext:\",lst[0].selftext) # This is the body of the post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58aa09b",
   "metadata": {},
   "source": [
    "Perhaps a more familiar way to interact with each item of this list is as a `dict`. Luckily, the API includes an easy way to get all of the available info as a dict without any effort - like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964b3bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Alexbuildit',\n",
       " 'created_utc': 1609541557,\n",
       " 'id': 'kol20h',\n",
       " 'num_comments': 2,\n",
       " 'score': 1,\n",
       " 'selftext': \"Brand new investor here. Saw all the hype surrounding GME, and bought in with this months paycheck. Had a good laugh when I saw the reddit award &lt; GME post. Already down over a hundred dollars on GME, but not gonna sell! Let's send GME to the moon! I'll keep picking up GME whenever I can. ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€\\n\\n[Yep. Checks out.](https://preview.redd.it/i6wj8ynnvs861.png?width=804&amp;format=png&amp;auto=webp&amp;s=69a0688bddce3cf409caea5adde58421e60296fc)\",\n",
       " 'subreddit': 'wallstreetbets',\n",
       " 'title': 'WSB In A Nutshell. Send GME To The Moon! ðŸš€',\n",
       " 'url': 'https://www.reddit.com/r/wallstreetbets/comments/kol20h/wsb_in_a_nutshell_send_gme_to_the_moon/',\n",
       " 'created': 1609566757.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst[0].d_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8956ee46",
   "metadata": {},
   "source": [
    "That's much better! \n",
    "\n",
    "However, you'll notice that the returned values for `created` and `created_utc` aren't particularly user-friendly. They're in the same UNIX-style epoch integer format we had to specify in the query. A quick way to add a human-readable version is a function like the below. You'll notice the human-readable timestamp added onto the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd7987cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'Alexbuildit',\n",
       " 'created_utc': 1609541557,\n",
       " 'id': 'kol20h',\n",
       " 'num_comments': 2,\n",
       " 'score': 1,\n",
       " 'selftext': \"Brand new investor here. Saw all the hype surrounding GME, and bought in with this months paycheck. Had a good laugh when I saw the reddit award &lt; GME post. Already down over a hundred dollars on GME, but not gonna sell! Let's send GME to the moon! I'll keep picking up GME whenever I can. ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€\\n\\n[Yep. Checks out.](https://preview.redd.it/i6wj8ynnvs861.png?width=804&amp;format=png&amp;auto=webp&amp;s=69a0688bddce3cf409caea5adde58421e60296fc)\",\n",
       " 'subreddit': 'wallstreetbets',\n",
       " 'title': 'WSB In A Nutshell. Send GME To The Moon! ðŸš€',\n",
       " 'url': 'https://www.reddit.com/r/wallstreetbets/comments/kol20h/wsb_in_a_nutshell_send_gme_to_the_moon/',\n",
       " 'created': 1609566757.0,\n",
       " 'datetime_utc': '2021-01-01T14:52:37'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_date(timestamp):\n",
    "    return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "lst[0].d_['datetime_utc'] = convert_date( lst[0].d_['created_utc'] )\n",
    "lst[0].d_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb59b2",
   "metadata": {},
   "source": [
    "Depending on the ticker, you may find A LOT of posts (if you don't assign a `limit` value, of course). One handy capability of the API is to filter based on fields so we can seach only for fields with at least N comments. Notice that we need to express the greater than as a string (`\">100\"`), which isn't totally obvious from the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4578ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'redcedar53',\n",
       " 'created_utc': 1609535773,\n",
       " 'id': 'kojagn',\n",
       " 'num_comments': 157,\n",
       " 'score': 1,\n",
       " 'selftext': 'January 4/5: Cohen discloses his additional 7% purchase of GME last Friday, brining his total ownership of GME to 20%. \\n\\nJanuary 6/7: GME announces Cohenâ€™s seat at the BoD as a special advisor to the modernization of GME. \\n\\nJanuary 9: GME releases December sales numbers.\\n\\nJanuary 11: The Conference where Papa Cohen himself presents his vision and roadmap for GME and gathers institutional buyers.\\n\\nTLDR: Next week pops to build up the momentum for the eventual rocket squeeze on the week of 11th.\\n\\n#NEW YEAR, NEW (G)ME ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€',\n",
       " 'subreddit': 'wallstreetbets',\n",
       " 'title': 'GMEâ€™s Game Plan Next Week (Probably)',\n",
       " 'url': 'https://www.reddit.com/r/wallstreetbets/comments/kojagn/gmes_game_plan_next_week_probably/',\n",
       " 'created': 1609560973.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = api.search_submissions(q='GME', after=start_epoch, before=end_epoch, # these are the unix-based timestamps to search between\n",
    "                             subreddit=['wallstreetbets','stocks'], \n",
    "                             filter=['id','url','author', 'title', 'score','subreddit','selftext','num_comments'], # list of fields to return\n",
    "                             num_comments=\">100\",\n",
    "                             limit = 2 # limit on the number of records returned\n",
    "                              ) \n",
    "lst = list(gen)\n",
    "item = lst[0]\n",
    "item.d_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d37a72e",
   "metadata": {},
   "source": [
    "### Sidebar: Getting Comments\n",
    "For our purposes, just the `submissions` offer ample amounts of material to analyze so I'm generally ignoring the `comments` underneath them, other than tracking the `num_comments` value. However, if you wanted to pull the comments for a given submission, you could do it like below. \n",
    "\n",
    "Note a few things:\n",
    "1. pass in the `id` property of the submission item as `link_id`. Also not totally clearly documented IMO\n",
    "2. The filter values are a little different because the fields available on a comment are not exactly the same as on a submission. The main changes to note is that url -> permalink and selftext -> body. Otherwise, they seem similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247d3bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created</th>\n",
       "      <th>d_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1617413524</td>\n",
       "      <td>gt7atx5</td>\n",
       "      <td>t1_ghtky8c</td>\n",
       "      <td>/r/wallstreetbets/comments/kojagn/gmes_game_pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.617439e+09</td>\n",
       "      <td>{'author': '[deleted]', 'body': '[removed]', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1611883171</td>\n",
       "      <td>gl6b32t</td>\n",
       "      <td>t1_ghriokl</td>\n",
       "      <td>/r/wallstreetbets/comments/kojagn/gmes_game_pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.611908e+09</td>\n",
       "      <td>{'author': '[deleted]', 'body': '[removed]', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LemniscateSideEight</td>\n",
       "      <td>No. This is wrong. He has to disclose options:...</td>\n",
       "      <td>1609724561</td>\n",
       "      <td>gi0rulb</td>\n",
       "      <td>t1_ghsugzo</td>\n",
       "      <td>/r/wallstreetbets/comments/kojagn/gmes_game_pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.609750e+09</td>\n",
       "      <td>{'author': 'LemniscateSideEight', 'body': 'No....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LemniscateSideEight</td>\n",
       "      <td>He does not. He does not care about peasants.</td>\n",
       "      <td>1609697542</td>\n",
       "      <td>ghz689z</td>\n",
       "      <td>t1_ghrhevd</td>\n",
       "      <td>/r/wallstreetbets/comments/kojagn/gmes_game_pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.609723e+09</td>\n",
       "      <td>{'author': 'LemniscateSideEight', 'body': 'He ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>possibly6</td>\n",
       "      <td>I remember seeing massive $1m plus orders for ...</td>\n",
       "      <td>1609654197</td>\n",
       "      <td>ghwq0cf</td>\n",
       "      <td>t1_ghrfiim</td>\n",
       "      <td>/r/wallstreetbets/comments/kojagn/gmes_game_pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>1.609679e+09</td>\n",
       "      <td>{'author': 'possibly6', 'body': 'I remember se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                               body  \\\n",
       "0            [deleted]                                          [removed]   \n",
       "1            [deleted]                                          [removed]   \n",
       "2  LemniscateSideEight  No. This is wrong. He has to disclose options:...   \n",
       "3  LemniscateSideEight      He does not. He does not care about peasants.   \n",
       "4            possibly6  I remember seeing massive $1m plus orders for ...   \n",
       "\n",
       "   created_utc       id   parent_id  \\\n",
       "0   1617413524  gt7atx5  t1_ghtky8c   \n",
       "1   1611883171  gl6b32t  t1_ghriokl   \n",
       "2   1609724561  gi0rulb  t1_ghsugzo   \n",
       "3   1609697542  ghz689z  t1_ghrhevd   \n",
       "4   1609654197  ghwq0cf  t1_ghrfiim   \n",
       "\n",
       "                                           permalink  score       subreddit  \\\n",
       "0  /r/wallstreetbets/comments/kojagn/gmes_game_pl...      1  wallstreetbets   \n",
       "1  /r/wallstreetbets/comments/kojagn/gmes_game_pl...      1  wallstreetbets   \n",
       "2  /r/wallstreetbets/comments/kojagn/gmes_game_pl...      1  wallstreetbets   \n",
       "3  /r/wallstreetbets/comments/kojagn/gmes_game_pl...      1  wallstreetbets   \n",
       "4  /r/wallstreetbets/comments/kojagn/gmes_game_pl...      1  wallstreetbets   \n",
       "\n",
       "        created                                                 d_  \n",
       "0  1.617439e+09  {'author': '[deleted]', 'body': '[removed]', '...  \n",
       "1  1.611908e+09  {'author': '[deleted]', 'body': '[removed]', '...  \n",
       "2  1.609750e+09  {'author': 'LemniscateSideEight', 'body': 'No....  \n",
       "3  1.609723e+09  {'author': 'LemniscateSideEight', 'body': 'He ...  \n",
       "4  1.609679e+09  {'author': 'possibly6', 'body': 'I remember se...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_lst = list(api.search_comments(link_id=item.id,\n",
    "                                        filter=['id','parent_id','permalink','author', 'title', \n",
    "                                                'subreddit','body','num_comments','score'],\n",
    "                                        limit=5))\n",
    "pd.DataFrame(comments_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce22da",
   "metadata": {},
   "source": [
    "## Building a Downloader\n",
    "\n",
    "With a basic understanding of the API and `psaw` wrapper, we can construct a simple downloader which downloads all submissions (with greater than n comments) for a one week time window on any stock ticker. Then, since we will probably want to avoid needing to call the API repeatedly for the same data, we will save it as a jsonlines file.\n",
    "\n",
    "If you're not familiar with `jsonlines`, it's well worth checking out. Note that, by default, jsonlines will append to the end of an existing file if one exists, or will create a file if one doesn't. Keep this in mind if running the same code on the same date/ticker repeatedly. It's probably easiest to assume the `jl` files have duplicates in them and to simply dedupe when reading back from disk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "037341a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "def get_submissions(symbol, end_date):\n",
    "    end_date = pd.to_datetime(end_date) #ensure it's a datetime object not string\n",
    "    end_epoch = int(end_date.timestamp())\n",
    "    start_epoch = int((end_date-pd.offsets.Week(1)).timestamp())\n",
    "    gen = api.search_submissions(q=f'${symbol}', after=start_epoch, before=end_epoch,\n",
    "                                subreddit=['wallstreetbets','stocks'], num_comments = \">10\",\n",
    "                                filter=['id','url','author', 'title', 'subreddit',\n",
    "                                        'num_comments','score','selftext'] ) \n",
    "\n",
    "    path = os.path.join(DATA_ROOT,f'{symbol}.jl')\n",
    "    with jsonlines.open(path, mode='a') as writer:\n",
    "        for item in gen:\n",
    "            item.d_['date_utc'] = convert_date(item.d_['created_utc'])\n",
    "            writer.write(item.d_)\n",
    "    return\n",
    "\n",
    "\n",
    "get_submissions('GME','2021-07-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c828f2",
   "metadata": {},
   "source": [
    "If we had a list of tickers that we wanted to get across a longer daterange, we could use some nested for loops like below to iterate through symbols and weeks. Running the below should take 15-20 minutes to complete so feel free to narrow the scope of tickers or dates if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c538055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4d15fe428a40e281fcf69da8a8334f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GME\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f34bd7efa04f25bb36cb9fd9eaa5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chad/opt/anaconda3/envs/reddit/lib/python3.6/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/Users/Chad/opt/anaconda3/envs/reddit/lib/python3.6/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "\n",
    "symbols = ['GME']#,'AMC','SPCE','TSLA']\n",
    "\n",
    "for symbol in tqdm(symbols):\n",
    "    print(symbol)\n",
    "    for date in tqdm(pd.date_range('2021-01-01','2021-10-31', freq='W')):\n",
    "        try:\n",
    "            get_submissions(symbol,date)\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2317e8",
   "metadata": {},
   "source": [
    "## Try it Out!\n",
    "\n",
    "Enough reading, already! The above code is available on colab at the link below. Feel free to try it out yourself.\n",
    "\n",
    "<a style=\"text-align: center;\" href=\"https://colab.research.google.com/drive/1yf4fje7IFhI0skAXddTUx0nsPbCgJgTc?usp=sharing\"><img src=\"images/colab.png\" title=\"ipynb on Colab\" /></a>\n",
    "\n",
    "\n",
    "You can modify the notebook however you'd like without risk of breaking it. I really hope that those interested will \"fork\" from my notebook (all you'll need is a Google Drive to save a copy of the file...) and extend it to answer your own questions through data. \n",
    "\n",
    "## Summary\n",
    "In this first post, we've made it through the heavy lifting of downloading data from the API and storing it in a usable format on disk. In the next segment, we will do some basic analysis on how spikes in Reddit traffic may signal risk of increased volatility in a given stock.\n",
    "\n",
    "\n",
    "## One last thing...\n",
    "If you've found this post useful or enlightening, please consider subscribing to the email list to be notified of future posts (email addresses will only be used for this purpose...). To subscribe, scroll to the top of this page and look at the right sidebar.\n",
    "\n",
    "You can also follow me on twitter ([__@data2alpha__](https://twitter.com/data2alpha)) and forward to a friend or colleague who may find this topic interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reddit",
   "language": "python",
   "name": "reddit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
